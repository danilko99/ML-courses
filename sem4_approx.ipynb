{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Семинар 4: аппроксимация Q-функции\n",
    "\n",
    "## Майнор ВШЭ, 14.02.2019\n",
    "\n",
    "Опрос на сегодня - https://goo.gl/forms/kVa5qtBLavZh0tN93."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этой тетрадке мы будем использовать библиотеку __tensorflow__ для обучения нейронной сети, хотя можно использвать и любую другую библиотеку. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем тестировать наши модели на классической задаче с перевернутым маятником."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x215649a1eb8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x21559d13940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\").env\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Глубокое Q-обучение: построение сети\n",
    "\n",
    "Так как описание состояния в задаче с маятником представляет собой не \"сырые\" признаки, а уже предобработанные (координаты, углы), нам не нужна для начала сложная архитектура, начнем с такой:\n",
    "\n",
    "![img](qlearning_scheme.png)\n",
    "\n",
    "Для начала попробуйте использовать только полносвязные слои (__L.Dense__) и линейные активационные функции. Сигмоиды и другие функции не будут работать с ненормализованными входными данными. Начиние с сети с 1-2 скрытыми слоями и 100-200 нейронами, а затем усложняйте сеть пока не будет достигнут приличный результат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed: Не найдена указанная процедура.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-f7df9df7f4c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mL\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInteractiveSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[1;31m# pylint: disable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[1;31m# pylint: enable=wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[1;31m# Protocol buffers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph_pb2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnode_def_pb2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary_pb2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\core\\framework\\graph_pb2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0m_b\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m<\u001b[0m\u001b[1;36m3\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'latin1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdescriptor\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_descriptor\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmessage\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_message\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mreflection\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_reflection\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\google\\protobuf\\descriptor.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m   \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m   \u001b[1;32mimport\u001b[0m \u001b[0muuid\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m   \u001b[1;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotobuf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_message\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m   \u001b[0m_USE_C_DESCRIPTORS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_message\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_USE_C_DESCRIPTORS'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: DLL load failed: Не найдена указанная процедура."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.layers as L\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession()\n",
    "keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "network = keras.models.Sequential()\n",
    "network.add(L.InputLayer(state_dim))\n",
    "### Ваш код здесь - строим сеть! ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_action(state, epsilon=0):\n",
    "    \"\"\"\n",
    "    sample actions with epsilon-greedy policy\n",
    "    recap: with p = epsilon pick random action, else pick action with highest Q(s,a)\n",
    "    \"\"\"\n",
    "    \n",
    "    q_values = network.predict(state[None])[0]\n",
    "    \n",
    "    ### Ваш код здесь - нужно выбрать действия e-жадно ###\n",
    "    # action = None\n",
    "            \n",
    "    return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert network.output_shape == (None, n_actions), \"please make sure your model maps state s -> [Q(s,a0), ..., Q(s, a_last)]\"\n",
    "assert network.layers[-1].activation == keras.activations.linear, \"please make sure you predict q-values without nonlinearity\"\n",
    "\n",
    "# test epsilon-greedy exploration\n",
    "s = env.reset()\n",
    "assert np.shape(get_action(s)) == (), \"please return just one action (integer)\"\n",
    "for eps in [0., 0.1, 0.5, 1.0]:\n",
    "    state_frequencies = np.bincount([get_action(s, epsilon=eps) for i in range(10000)], minlength=n_actions)\n",
    "    best_action = state_frequencies.argmax()\n",
    "    assert abs(state_frequencies[best_action] - 10000 * (1 - eps + eps / n_actions)) < 200\n",
    "    for other_action in range(n_actions):\n",
    "        if other_action != best_action:\n",
    "            assert abs(state_frequencies[other_action] - 10000 * (eps / n_actions)) < 200\n",
    "    print('e=%.1f tests passed'%eps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-обучение через градиентный спуск\n",
    "\n",
    "Теперь будем приближать Q-функцию агента, минимизируя TD функцию потерь:\n",
    "$$ L = { 1 \\over N} \\sum_i (Q_{\\theta}(s,a) - [r(s,a) + \\gamma \\cdot max_{a'} Q_{-}(s', a')]) ^2. $$\n",
    "\n",
    "Основная тонкость состоит в использовани  $Q_{-}(s',a')$. Эта таже самая функция, что и $Q_{\\theta}$, которая является выходомо нейронной сети, но при обучении сети, мы не пропускаем через эти слои градиенты. Ддля этого используется функция `tf.stop_gradient`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create placeholders for the <s, a, r, s'> tuple and a special indicator for game end (is_done = True)\n",
    "states_ph = tf.placeholder('float32', shape=(None,) + state_dim)\n",
    "actions_ph = tf.placeholder('int32', shape=[None])\n",
    "rewards_ph = tf.placeholder('float32', shape=[None])\n",
    "next_states_ph = tf.placeholder('float32', shape=(None,) + state_dim)\n",
    "is_done_ph = tf.placeholder('bool', shape=[None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#get q-values for all actions in current states\n",
    "predicted_qvalues = network(states_ph)\n",
    "\n",
    "#select q-values for chosen actions\n",
    "predicted_qvalues_for_actions = tf.reduce_sum(predicted_qvalues * tf.one_hot(actions_ph, n_actions), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "\n",
    "# compute q-values for all actions in next states\n",
    "### Ваш код здесь - применяем сеть для получения q-value для next_states_ph ###\n",
    "# predicted_next_qvalues =\n",
    "\n",
    "### Ваш код здесь - вычисляем V*(next_states) по предсказанным следующим q-values ###\n",
    "# next_state_values = \n",
    "\n",
    "### Ваш код здесь - вычисляем target q-values для функции потерь ###\n",
    "# target_qvalues_for_actions = \n",
    "\n",
    "# at the last state we shall use simplified formula: Q(s,a) = r(s,a) since s' doesn't exist\n",
    "target_qvalues_for_actions = tf.where(is_done_ph, rewards_ph, target_qvalues_for_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Ваш код здесь - среднеквадратичная функция потерь stop_gradient!###\n",
    "# loss = \n",
    "\n",
    "\n",
    "# training function that resembles agent.update(state, action, reward, next_state) from tabular agent\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert tf.gradients(loss, [predicted_qvalues_for_actions])[0] is not None, \"make sure you update q-values for chosen actions and not just all actions\"\n",
    "assert tf.gradients(loss, [predicted_next_qvalues])[0] is None, \"make sure you don't propagate gradient w.r.t. Q_(s',a')\"\n",
    "assert predicted_next_qvalues.shape.ndims == 2, \"make sure you predicted q-values for all actions in next state\"\n",
    "assert next_state_values.shape.ndims == 1, \"make sure you computed V(s') as maximum over just the actions axis and not all axes\"\n",
    "assert target_qvalues_for_actions.shape.ndims == 1, \"there's something wrong with target q-values, they must be a vector\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_session(t_max=1000, epsilon=0, train=False):\n",
    "    \"\"\"play env with approximate q-learning agent and train it at the same time\"\"\"\n",
    "    total_reward = 0\n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        a = get_action(s, epsilon=epsilon)       \n",
    "        next_s, r, done, _ = env.step(a)\n",
    "        \n",
    "        if train:\n",
    "            sess.run(train_step,{\n",
    "                states_ph: [s], actions_ph: [a], rewards_ph: [r], \n",
    "                next_states_ph: [next_s], is_done_ph: [done]\n",
    "            })\n",
    "\n",
    "        total_reward += r\n",
    "        s = next_s\n",
    "        if done: break\n",
    "            \n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epsilon = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    session_rewards = [generate_session(epsilon=epsilon, train=True) for _ in range(100)]\n",
    "    print(\"epoch #{}\\tmean reward = {:.3f}\\tepsilon = {:.3f}\".format(i, np.mean(session_rewards), epsilon))\n",
    "    \n",
    "    epsilon *= 0.99\n",
    "    assert epsilon >= 1e-4, \"Make sure epsilon is always nonzero during training\"\n",
    "    \n",
    "    if np.mean(session_rewards) > 300:\n",
    "        print (\"You Win!\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Интерпретация результатов\n",
    "\n",
    "\n",
    "* __ mean reward__  - средне вознаграждеие за эпизод. в Случае корреткной реализации, этот показатель будет низким 10 эпох и только затем будет возрастать и сойдется на 50-100 шагов в зависииости от архитектуры сети.\n",
    "* Если сеть ен достигает нужных результатов к концу цикла, попробуйте увеличить число нейронов в скрытом слое или поменяйте $\\epsion$.\n",
    "* __ epsilon__ обеспечивает стремление агента исследовать среду. Можно искусственно увеличвать малые значения $\\epsilon$ при низких результатаз до 0.1 - 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Запись роликов\n",
    "\n",
    "Можно использовать `gym.wrappers.Monitor` для записи сессий агента. \n",
    "\n",
    "Для финальной пробы агента, мы будем ставить  epsilon=0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#record sessions\n",
    "import gym.wrappers\n",
    "env = gym.wrappers.Monitor(gym.make(\"CartPole-v0\"),directory=\"videos\",force=True)\n",
    "sessions = [generate_session(epsilon=0, train=False) for _ in range(100)]\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
