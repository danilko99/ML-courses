{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E7ME-ag0hoDA"
   },
   "source": [
    "# Данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-08T10:37:05.907196Z",
     "start_time": "2019-06-08T10:37:02.404297Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "colab_type": "code",
    "id": "eVLBU4s9fF-q",
    "outputId": "3116fcaf-0d65-4c62-c4fa-b1070be5aa5e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2019-06-08 13:37:02--  https://www.dropbox.com/s/nd7v1fod89xla6j/vk_texts_with_sources.csv\n",
      "Распознаётся www.dropbox.com (www.dropbox.com)… 162.125.70.1, 2620:100:6026:1::a27d:4601\n",
      "Подключение к www.dropbox.com (www.dropbox.com)|162.125.70.1|:443... соединение установлено.\n",
      "HTTP-запрос отправлен. Ожидание ответа… 301 Moved Permanently\n",
      "Адрес: /s/raw/nd7v1fod89xla6j/vk_texts_with_sources.csv [переход]\n",
      "--2019-06-08 13:37:02--  https://www.dropbox.com/s/raw/nd7v1fod89xla6j/vk_texts_with_sources.csv\n",
      "Повторное использование соединения с www.dropbox.com:443.\n",
      "HTTP-запрос отправлен. Ожидание ответа… 302 Found\n",
      "Адрес: https://ucef8ec168b1b06e78793503c3dc.dl.dropboxusercontent.com/cd/0/inline/AiZI8gFnr2mz4AgVNUNr8BksATY3YToGVdYckdntYCOQkRxIcAwXv-MpObGhO3O6959SYVOTluEAo0yjdV1uJVUyp3qdsEexC8ZFl9seLAqoUA/file# [переход]\n",
      "--2019-06-08 13:37:03--  https://ucef8ec168b1b06e78793503c3dc.dl.dropboxusercontent.com/cd/0/inline/AiZI8gFnr2mz4AgVNUNr8BksATY3YToGVdYckdntYCOQkRxIcAwXv-MpObGhO3O6959SYVOTluEAo0yjdV1uJVUyp3qdsEexC8ZFl9seLAqoUA/file\n",
      "Распознаётся ucef8ec168b1b06e78793503c3dc.dl.dropboxusercontent.com (ucef8ec168b1b06e78793503c3dc.dl.dropboxusercontent.com)… 162.125.70.6, 2620:100:6026:6::a27d:4606\n",
      "Подключение к ucef8ec168b1b06e78793503c3dc.dl.dropboxusercontent.com (ucef8ec168b1b06e78793503c3dc.dl.dropboxusercontent.com)|162.125.70.6|:443... соединение установлено.\n",
      "HTTP-запрос отправлен. Ожидание ответа… 200 OK\n",
      "Длина: 14406837 (14M) [text/plain]\n",
      "Сохранение в: «vk_texts_with_sources.csv»\n",
      "\n",
      "vk_texts_with_sourc 100%[===================>]  13,74M  8,14MB/s    за 1,7s    \n",
      "\n",
      "2019-06-08 13:37:05 (8,14 MB/s) - «vk_texts_with_sources.csv» сохранён [14406837/14406837]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://www.dropbox.com/s/nd7v1fod89xla6j/vk_texts_with_sources.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-08T18:06:18.394677Z",
     "start_time": "2019-06-08T18:06:18.390453Z"
    }
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import re\n",
    "\n",
    "import fasttext\n",
    "from gensim.models import Word2Vec, Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from lightgbm import LGBMClassifier\n",
    "import nltk\n",
    "import pymorphy2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pymystem3 import Mystem\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-08T18:06:18.933752Z",
     "start_time": "2019-06-08T18:06:18.803335Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "id": "qxZMU8CBK664",
    "outputId": "ad5442ea-27c3-4357-8ab9-6249143c49ed"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Начальник Главного оперативного управления Ген...</td>\n",
       "      <td>mil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Артиллерийские подразделения общевойскового об...</td>\n",
       "      <td>mil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Подразделения морской пехоты Каспийской флотил...</td>\n",
       "      <td>mil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Команды на всеармейских этапах конкурсов АрМИ-...</td>\n",
       "      <td>mil</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>На большом учебно-методическом командирском сб...</td>\n",
       "      <td>mil</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text source\n",
       "0  Начальник Главного оперативного управления Ген...    mil\n",
       "1  Артиллерийские подразделения общевойскового об...    mil\n",
       "2  Подразделения морской пехоты Каспийской флотил...    mil\n",
       "3  Команды на всеармейских этапах конкурсов АрМИ-...    mil\n",
       "4  На большом учебно-методическом командирском сб...    mil"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('vk_texts_with_sources.csv', usecols = ['text', 'source'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dYNCAr17MFA3"
   },
   "source": [
    "# Домашнее задание\n",
    "\n",
    "В этом домашнем задании вы будете решать задачу тематической классификации. Даны тексты, опубликованные в нескольких пабликах VK.com, посвященных государственным и муниципальным службам. Формально задача заключается в том, чтобы по тексту ($d$) определить в каком паблике он опубликован, то есть, к какому классу $c$ он принадлежит. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e2EnN-U7MemU"
   },
   "source": [
    "## Задание 1 [1 балл]. Описательные статистики\n",
    "Посчитайте:\n",
    "* количество текстов и количество классов\n",
    "* количество слов (без лемматизации и с лемматизацией) в коллекции\n",
    "* среднюю длину текста в словах и символах\n",
    "* найдите 5 самых частых существительных в текстах каждого паблика \n",
    "\n",
    "*Рекомендуем использовать pandas для расчета описательных статистик.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-08T18:06:19.758182Z",
     "start_time": "2019-06-08T18:06:19.661239Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количечтво пустых текстов: 99\n",
      "Количество текстов с дубликатами: 11526\n",
      "Количество текстов без дубликатов: 11153\n",
      "Количество уникальных текстов: 11149\n"
     ]
    }
   ],
   "source": [
    "print('Количечтво пустых текстов:', df['text'].isna().sum())\n",
    "df = df.dropna()\n",
    "\n",
    "print('Количество текстов с дубликатами:', df.shape[0])\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "print('Количество текстов без дубликатов:', df.shape[0])\n",
    "print('Количество уникальных текстов:', df['text'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-08T12:11:25.397417Z",
     "start_time": "2019-06-08T12:11:25.387615Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество классов: 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "mil            2988\n",
       "mospolice      2792\n",
       "mchsgov        2784\n",
       "russianpost    2589\n",
       "Name: source, dtype: int64"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Количество классов:', df['source'].nunique())\n",
    "df['source'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-08T12:11:40.860518Z",
     "start_time": "2019-06-08T12:11:25.751739Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество слов без лемматизации в коллекции: 96746\n",
      "Количество слов c лемматизацией в коллекции: 31489\n"
     ]
    }
   ],
   "source": [
    "word_collection = set()\n",
    "df['text'].apply(lambda text: word_collection.update(set(nltk.tokenize.word_tokenize(text))))    \n",
    "print('Количество слов без лемматизации в коллекции:', len(word_collection))\n",
    "\n",
    "mystem_analyzer = Mystem()\n",
    "lemmatize_word_collection = set()\n",
    "for word in word_collection:\n",
    "    lemmatize_word_collection.add(mystem_analyzer.lemmatize(word)[0])\n",
    "    \n",
    "print('Количество слов c лемматизацией в коллекции:', len(lemmatize_word_collection))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-08T12:11:50.052080Z",
     "start_time": "2019-06-08T12:11:41.070088Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Средняя длина текста в словах: 122.69765982246929\n",
      "Средняя длина текста в символах: 711.1856899488927\n"
     ]
    }
   ],
   "source": [
    "print('Средняя длина текста в словах:', np.mean([len(nltk.tokenize.word_tokenize(text)) \n",
    "                                                for text in df['text']]))\n",
    "print('Средняя длина текста в символах:', np.mean([len(text) for text in df['text']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-08T13:23:06.716111Z",
     "start_time": "2019-06-08T13:19:31.363489Z"
    },
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 in mil - минобороны, военный, россия, год, оборона\n",
      "Top 5 in mchsgov - мчс, мчсроссия, россия, спасатель, человек\n",
      "Top 5 in russianpost - почта, россия, год, отделение, письмо\n",
      "Top 5 in mospolice - полиция, мвд, москва, россия, сотрудник\n"
     ]
    }
   ],
   "source": [
    "morph = pymorphy2.MorphAnalyzer()\n",
    "for source in df['source'].unique():\n",
    "    counter = collections.Counter()\n",
    "    for text in df[df['source'] == source]['text']:\n",
    "        for word in nltk.tokenize.word_tokenize(text): \n",
    "            parsed = morph.parse(word)[0]\n",
    "            if 'NOUN' in parsed.tag:\n",
    "                counter.update([parsed.normal_form])\n",
    "    print('Top 5 in', source, '-', ', '.join([word[0] for word in counter.most_common(5)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fTJPlulvQSqQ"
   },
   "source": [
    "Разделите коллекцию текстов на обучающую и тестовую части. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-08T18:06:57.440094Z",
     "start_time": "2019-06-08T18:06:21.687337Z"
    }
   },
   "outputs": [],
   "source": [
    "m = Mystem()\n",
    "regex = re.compile(\"[А-Яа-я:=!\\)\\()A-z\\_\\%/|]+\")\n",
    "\n",
    "def words_only(text, regex=regex):\n",
    "    return \" \".join(regex.findall(text)).replace('<br>', '')\n",
    "\n",
    "def lemmatize(text, mystem=m):\n",
    "    return \"\".join(m.lemmatize(text)).strip()\n",
    "\n",
    "df_prep = df.copy()\n",
    "df_prep.text = df.text.apply(words_only)\n",
    "df_prep.text = df_prep.text.apply(lemmatize)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_prep.drop('source', axis=1), \n",
    "                                                    df_prep['source'], test_size=0.2, \n",
    "                                                    random_state=42, stratify=df_prep['source'])\n",
    "del df_prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9yCVYfbpNhXd"
   },
   "source": [
    " ## Задание 2 [2 балла]. Классификация по правилам\n",
    " \n",
    " * Разработайте несколько правил вида \"Если встречается слово $w$, то текст относится к паблику $c$\"\n",
    " * Посчитайте, какую точность, полноту, $f$-меру и $accuracy$ вы получаете при классификации по правилам\n",
    " * Получилось ли у вас придумать правило, которое никогда не ошибается?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-08T14:24:14.054119Z",
     "start_time": "2019-06-08T14:24:04.839736Z"
    }
   },
   "outputs": [],
   "source": [
    "word_sets = {}\n",
    "len_sets = {}\n",
    "for source in df['source'].unique():\n",
    "    word_sets[source] = set()\n",
    "    len_sets[source] = set()\n",
    "    for text in df[df['source'] == source]['text']:\n",
    "        word_sets[source].update(nltk.tokenize.word_tokenize(text))\n",
    "        len_sets[source].add(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-08T14:24:15.124659Z",
     "start_time": "2019-06-08T14:24:15.058712Z"
    }
   },
   "outputs": [],
   "source": [
    "# запоминаем уникальные для каждого паблика уникальные слова и длины текстов\n",
    "unique_word_sets = {}\n",
    "unique_len_sets = {}\n",
    "for source in word_sets.keys():\n",
    "    diff_sources = set(word_sets.keys())\n",
    "    diff_sources.remove(source)\n",
    "    diff_word_set = set()\n",
    "    diff_len_set = set()\n",
    "    for s in diff_sources:\n",
    "        diff_word_set.update(word_sets[s])\n",
    "        diff_len_set.update(len_sets[s])\n",
    "    unique_word_sets[source] = word_sets[source] - diff_word_set\n",
    "    unique_len_sets[source] = len_sets[source] - diff_len_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-08T14:24:30.321767Z",
     "start_time": "2019-06-08T14:24:20.323566Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa9c199261194c1180a6da7e742f94d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=11153), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rand = 0\n",
    "y_pred = []\n",
    "for text in tqdm.tqdm_notebook(df['text']):\n",
    "    has_in_sets = False\n",
    "    tekenized_text = nltk.tokenize.word_tokenize(text)\n",
    "    for key in unique_word_sets.keys():\n",
    "        if not has_in_sets:\n",
    "            if unique_word_sets[key] & set(tekenized_text) or \\\n",
    "                    len(text) in unique_len_sets[key]:\n",
    "                y_pred.append(key)\n",
    "                has_in_sets = True\n",
    "                break\n",
    "                \n",
    "    if not has_in_sets:\n",
    "        rand += 1\n",
    "        y_pred.append(df['source'].unique()[np.random.randint(0,4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-08T14:24:31.332575Z",
     "start_time": "2019-06-08T14:24:31.326406Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Случайных ответов: 304\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array(['mchsgov', 'mil', 'mospolice', 'russianpost'], dtype='<U11'),\n",
       " array([2664, 3049, 2817, 2623]))"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Случайных ответов:', rand)\n",
    "np.unique(y_pred, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-08T14:24:45.123527Z",
     "start_time": "2019-06-08T14:24:45.008276Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score: 0.980095041692818\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     mchsgov       0.99      0.95      0.97      2784\n",
      "         mil       0.98      1.00      0.99      2988\n",
      "   mospolice       0.98      0.99      0.98      2792\n",
      " russianpost       0.98      0.99      0.98      2589\n",
      "\n",
      "   micro avg       0.98      0.98      0.98     11153\n",
      "   macro avg       0.98      0.98      0.98     11153\n",
      "weighted avg       0.98      0.98      0.98     11153\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('accuracy_score:', accuracy_score(df['source'], y_pred))\n",
    "print(classification_report(df['source'], y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**При написании правил для всех текстов точность высокая. Падает до 70%, если разбить на трейн-тест**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "22tJKQnaOjie"
   },
   "source": [
    "## Задание 3 [3 балла]. Baseline\n",
    "Используйте стандартный ```sklearn.pipeline``` для классификации текстов: \n",
    "* векторизация \n",
    "* $tf-idf$ взвешивание \n",
    "* ваш любимый метод классификации.\n",
    "\n",
    "\n",
    "Оцените результаты классификации по стандартным мерам качества и проведите анализ ошибок. Для этого рекомендуем визуализировать матрицу ошибок (confusion matrix). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-08T15:09:17.006709Z",
     "start_time": "2019-06-08T15:08:58.880894Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([('tfidf', TfidfVectorizer(token_pattern=r'\\b\\w+\\b')),\n",
    "                 ('lgb', LGBMClassifier())])\n",
    "\n",
    "pipe.fit(X_train['text'], y_train)\n",
    "y_pred = pipe.predict(X_test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-08T15:09:18.659332Z",
     "start_time": "2019-06-08T15:09:18.623091Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score: 0.974002689376961\n",
      "classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     mchsgov       0.98      0.95      0.97       557\n",
      "         mil       0.98      0.99      0.98       598\n",
      "   mospolice       0.98      0.99      0.98       558\n",
      " russianpost       0.96      0.97      0.96       518\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      2231\n",
      "   macro avg       0.97      0.97      0.97      2231\n",
      "weighted avg       0.97      0.97      0.97      2231\n",
      "\n",
      "confusion_matrix:\n",
      "[[530   7   0  20]\n",
      " [  7 590   0   1]\n",
      " [  1   2 553   2]\n",
      " [  3   2  13 500]]\n"
     ]
    }
   ],
   "source": [
    "print('accuracy_score:', accuracy_score(y_test, y_pred))\n",
    "print('classification_report:', classification_report(y_test, y_pred), sep='\\n')\n",
    "print('confusion_matrix:', confusion_matrix(y_test, y_pred), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Качество очень высокое. 1 и 3, и 2 и 3 классы разделяются лучше всех**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I4m1rDQ3PAqO"
   },
   "source": [
    "## Задание 4 [2 балла]. Снижение размерности\n",
    "Добавьте в ваш ```sklearn.pipeline```  методы снижения размерности:  PCA / LSI / LSA / LDA / другое. Какие методы классификации разумно использовать после снижения размерности? Как изменились результаты классификации после добавления нового шага?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-08T15:09:23.089449Z",
     "start_time": "2019-06-08T15:09:20.223556Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-2e70b659bc62>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m pipe = Pipeline([('tfidf', TfidfVectorizer(token_pattern=r'\\b\\w+\\b')),\n\u001b[0m\u001b[0;32m      2\u001b[0m                  \u001b[1;33m(\u001b[0m\u001b[1;34m'svd'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTruncatedSVD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                  ('lgb', LGBMClassifier())])\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([('tfidf', TfidfVectorizer(token_pattern=r'\\b\\w+\\b')),\n",
    "                 ('svd', TruncatedSVD(n_components=10)),\n",
    "                 ('lgb', LGBMClassifier())])\n",
    "\n",
    "pipe.fit(X_train['text'], y_train)\n",
    "y_pred = pipe.predict(X_test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-08T15:09:24.702678Z",
     "start_time": "2019-06-08T15:09:24.662036Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score: 0.9641416405199462\n",
      "classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     mchsgov       0.97      0.95      0.96       557\n",
      "         mil       0.98      0.97      0.98       598\n",
      "   mospolice       0.97      0.99      0.98       558\n",
      " russianpost       0.94      0.94      0.94       518\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      2231\n",
      "   macro avg       0.96      0.96      0.96      2231\n",
      "weighted avg       0.96      0.96      0.96      2231\n",
      "\n",
      "confusion_matrix:\n",
      "[[529   6   1  21]\n",
      " [  7 583   1   7]\n",
      " [  3   0 551   4]\n",
      " [  8   5  17 488]]\n"
     ]
    }
   ],
   "source": [
    "print('accuracy_score:', accuracy_score(y_test, y_pred))\n",
    "print('classification_report:', classification_report(y_test, y_pred), sep='\\n')\n",
    "print('confusion_matrix:', confusion_matrix(y_test, y_pred), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Какие методы классификации разумно использовать после снижения размерности?\n",
    "- Любые модели, которым нужно меньше признаков\n",
    "- Любые модели, которые не любят коррелирующие признаки\n",
    "\n",
    "Как изменились результаты классификации после добавления нового шага?\n",
    " - Метрики ухудшиласилс\n",
    " - Ошибки предсказания стали более размазаны в матрице ошибок"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7vVPaaVwPVwg"
   },
   "source": [
    "## Задание 5 [1 балл]. Лемматизация\n",
    "Посмотрите, как влияет лемматизация на качество классификации. Как изменится качество классификации, если вы используете ```CountVectorizer``` на словах или $n$-граммах на лемматизированных текстах?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-08T15:09:39.537524Z",
     "start_time": "2019-06-08T15:09:26.268580Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([('tfidf', TfidfVectorizer(ngram_range=(1, 3), token_pattern=r'\\b\\w+\\b')),\n",
    "                 ('svd', TruncatedSVD(n_components=10)),\n",
    "                 ('lgb', LGBMClassifier())])\n",
    "\n",
    "pipe.fit(X_train['text'], y_train)\n",
    "y_pred = pipe.predict(X_test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-08T15:09:41.355968Z",
     "start_time": "2019-06-08T15:09:41.318844Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score: 0.9605558045719408\n",
      "classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     mchsgov       0.97      0.92      0.95       557\n",
      "         mil       0.97      0.97      0.97       598\n",
      "   mospolice       0.96      0.99      0.97       558\n",
      " russianpost       0.94      0.96      0.95       518\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      2231\n",
      "   macro avg       0.96      0.96      0.96      2231\n",
      "weighted avg       0.96      0.96      0.96      2231\n",
      "\n",
      "confusion_matrix:\n",
      "[[515  10   5  27]\n",
      " [  8 583   3   4]\n",
      " [  1   4 550   3]\n",
      " [  6   1  16 495]]\n"
     ]
    }
   ],
   "source": [
    "print('accuracy_score:', accuracy_score(y_test, y_pred))\n",
    "print('classification_report:', classification_report(y_test, y_pred), sep='\\n')\n",
    "print('confusion_matrix:', confusion_matrix(y_test, y_pred), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-08T15:12:35.505658Z",
     "start_time": "2019-06-08T15:12:08.751839Z"
    }
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline([('tfidf', TfidfVectorizer(ngram_range=(1, 2), token_pattern=r'\\b\\w+\\b')),\n",
    "                 ('lgb', LGBMClassifier())])\n",
    "\n",
    "pipe.fit(X_train['text'], y_train)\n",
    "y_pred = pipe.predict(X_test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-08T15:12:37.151163Z",
     "start_time": "2019-06-08T15:12:37.113974Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score: 0.9748991483639623\n",
      "classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     mchsgov       0.98      0.96      0.97       557\n",
      "         mil       0.98      0.99      0.98       598\n",
      "   mospolice       0.98      0.99      0.98       558\n",
      " russianpost       0.96      0.96      0.96       518\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      2231\n",
      "   macro avg       0.97      0.97      0.97      2231\n",
      "weighted avg       0.97      0.97      0.97      2231\n",
      "\n",
      "confusion_matrix:\n",
      "[[532   7   1  17]\n",
      " [  7 591   0   0]\n",
      " [  0   2 554   2]\n",
      " [  4   4  12 498]]\n"
     ]
    }
   ],
   "source": [
    "print('accuracy_score:', accuracy_score(y_test, y_pred))\n",
    "print('classification_report:', classification_report(y_test, y_pred), sep='\\n')\n",
    "print('confusion_matrix:', confusion_matrix(y_test, y_pred), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-08T14:55:13.247214Z",
     "start_time": "2019-06-08T14:55:13.241595Z"
    }
   },
   "source": [
    "**При использовании n-грамм без использования понижения размерности качество выросло**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cdakRHahQp-l"
   },
   "source": [
    "## Задание 6 [3 балла]. Continious bag of words\n",
    "Для baseline решения мы использовали обычное представление текста в виде мешка слов. Попробуйте использовать другие модели представления текста – например, в виде непрерывного мешка слов, то есть, в виде набора эмбеддингов. Для того, чтобы получить вектор текста попробуйте:\n",
    "* усреднить все эмбеддинги слов, входящих в этот текст\n",
    "* усреднить все эмбеддинги слов, входящих в этот текст с $tf-idf$ весами\n",
    "* использовать любую модель эмбеддинга документа.\n",
    "\n",
    "Используйте любую модель эмбеддингов по вашему вкусу. \n",
    "\n",
    "\n",
    "Оцените результаты классификации по стандартным мерам качества и проведите анализ ошибок. Для этого рекомендуем визуализировать матрицу ошибок (confusion matrix). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-08T17:37:09.422385Z",
     "start_time": "2019-06-08T17:37:01.873142Z"
    }
   },
   "outputs": [],
   "source": [
    "texts = [pd.concat([X_train, X_test])['text'].iloc[i].split() for i in range(len(df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-08T17:39:22.115901Z",
     "start_time": "2019-06-08T17:39:18.363082Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='\\\\b\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model = Word2Vec(texts, size=100, window=5, min_count=5, workers=4)\n",
    "tfidf = TfidfVectorizer(token_pattern=r'\\b\\w+\\b')\n",
    "tfidf.fit(X_train['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-08T15:36:04.362521Z",
     "start_time": "2019-06-08T15:36:04.357455Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_emb(data, tfidf=None):\n",
    "    emb = []\n",
    "    for text in data['text']:\n",
    "        text_emb = []\n",
    "        for word in text:\n",
    "            try:\n",
    "                if tfidf is None:\n",
    "                    text_emb.append(w2v_model.wv.get_vector(word))\n",
    "                else:\n",
    "                    text_emb.append(w2v_model.wv.get_vector(word) * tfidf.vocabulary_[word])\n",
    "            except KeyError:\n",
    "                pass\n",
    "        emb.append(np.mean(text_emb, axis=0))\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-08T15:21:51.188713Z",
     "start_time": "2019-06-08T15:21:48.909937Z"
    }
   },
   "outputs": [],
   "source": [
    "train_emb = get_emb(X_train)\n",
    "test_emb = get_emb(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-08T15:24:41.421775Z",
     "start_time": "2019-06-08T15:24:38.219520Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score: 0.789332138054684\n",
      "classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     mchsgov       0.86      0.77      0.81       557\n",
      "         mil       0.80      0.84      0.82       598\n",
      "   mospolice       0.81      0.80      0.81       558\n",
      " russianpost       0.69      0.73      0.71       518\n",
      "\n",
      "   micro avg       0.79      0.79      0.79      2231\n",
      "   macro avg       0.79      0.79      0.79      2231\n",
      "weighted avg       0.79      0.79      0.79      2231\n",
      "\n",
      "confusion_matrix:\n",
      "[[430  29  23  75]\n",
      " [ 22 503  32  41]\n",
      " [ 16  39 448  55]\n",
      " [ 31  55  52 380]]\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([('lgb', LGBMClassifier())])\n",
    "\n",
    "pipe.fit(train_emb, y_train)\n",
    "y_pred = pipe.predict(test_emb)\n",
    "\n",
    "print('accuracy_score:', accuracy_score(y_test, y_pred))\n",
    "print('classification_report:', classification_report(y_test, y_pred), sep='\\n')\n",
    "print('confusion_matrix:', confusion_matrix(y_test, y_pred), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-08T15:36:30.015977Z",
     "start_time": "2019-06-08T15:36:05.152452Z"
    }
   },
   "outputs": [],
   "source": [
    "train_emb = get_emb(X_train, tfidf=tfidf)\n",
    "test_emb = get_emb(X_test, tfidf=tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-08T15:37:17.223672Z",
     "start_time": "2019-06-08T15:37:12.474457Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score: 0.7785746302106679\n",
      "classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     mchsgov       0.86      0.78      0.82       557\n",
      "         mil       0.79      0.80      0.80       598\n",
      "   mospolice       0.78      0.78      0.78       558\n",
      " russianpost       0.69      0.75      0.72       518\n",
      "\n",
      "   micro avg       0.78      0.78      0.78      2231\n",
      "   macro avg       0.78      0.78      0.78      2231\n",
      "weighted avg       0.78      0.78      0.78      2231\n",
      "\n",
      "confusion_matrix:\n",
      "[[434  24  24  75]\n",
      " [ 22 478  55  43]\n",
      " [ 13  57 434  54]\n",
      " [ 36  45  46 391]]\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([('lgb', LGBMClassifier())])\n",
    "\n",
    "pipe.fit(train_emb, y_train)\n",
    "y_pred = pipe.predict(test_emb)\n",
    "\n",
    "print('accuracy_score:', accuracy_score(y_test, y_pred))\n",
    "print('classification_report:', classification_report(y_test, y_pred), sep='\\n')\n",
    "print('confusion_matrix:', confusion_matrix(y_test, y_pred), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-08T16:08:01.778287Z",
     "start_time": "2019-06-08T16:07:56.454218Z"
    }
   },
   "outputs": [],
   "source": [
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(texts)]\n",
    "model = Doc2Vec(documents, vector_size=100, window=5, min_count=5, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-08T16:08:16.778052Z",
     "start_time": "2019-06-08T16:08:09.761278Z"
    }
   },
   "outputs": [],
   "source": [
    "train_vec = []\n",
    "for text in texts[:X_train.shape[0]]:\n",
    "    train_vec.append(model.infer_vector(texts[0]))\n",
    "test_vec = []\n",
    "for text in texts[X_train.shape[0]:]:\n",
    "    test_vec.append(model.infer_vector(texts[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-08T16:08:27.561055Z",
     "start_time": "2019-06-08T16:08:24.893273Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score: 0.2478709099058718\n",
      "classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     mchsgov       0.24      0.23      0.23       557\n",
      "         mil       0.25      0.33      0.29       598\n",
      "   mospolice       0.25      0.25      0.25       558\n",
      " russianpost       0.24      0.17      0.20       518\n",
      "\n",
      "   micro avg       0.25      0.25      0.25      2231\n",
      "   macro avg       0.25      0.25      0.24      2231\n",
      "weighted avg       0.25      0.25      0.24      2231\n",
      "\n",
      "confusion_matrix:\n",
      "[[126 210 129  92]\n",
      " [151 199 154  94]\n",
      " [122 201 138  97]\n",
      " [132 174 122  90]]\n"
     ]
    }
   ],
   "source": [
    "pipe = Pipeline([('lgb', LGBMClassifier())])\n",
    "\n",
    "pipe.fit(train_vec, y_train)\n",
    "y_pred = pipe.predict(test_vec)\n",
    "\n",
    "print('accuracy_score:', accuracy_score(y_test, y_pred))\n",
    "print('classification_report:', classification_report(y_test, y_pred), sep='\\n')\n",
    "print('confusion_matrix:', confusion_matrix(y_test, y_pred), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Наблюдения**\n",
    " - Doc2Vec показал очень плохие результаты, результаты случайны\n",
    " - Word2Vec показал результаты хуже обычного tf-idf\n",
    " - Word2Vec * tf-idf результат слегка ухудшился"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uyVQ5Gm7Qzcz"
   },
   "source": [
    "## Задание 7 [2 балла]. fastText\n",
    "\n",
    "Используйте ```fastText``` в режиме классификации. Оцените результаты классификации по стандартным мерам качества и проведите анализ ошибок. Для этого рекомендуем визуализировать матрицу ошибок (confusion matrix). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-08T16:54:57.286432Z",
     "start_time": "2019-06-08T16:54:56.693023Z"
    }
   },
   "outputs": [],
   "source": [
    "rows = []\n",
    "for idx, text in enumerate(X_train['text']):\n",
    "    rows.append(text + ' __label__' + y_train.iloc[idx])\n",
    "\n",
    "np.savetxt('data.train.txt', rows, fmt='%s')\n",
    "\n",
    "rows = []\n",
    "for idx, text in enumerate(X_test['text']):\n",
    "    rows.append(text)\n",
    "\n",
    "np.savetxt('data.test.txt', rows, fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-08T16:57:02.865753Z",
     "start_time": "2019-06-08T16:57:02.377836Z"
    }
   },
   "outputs": [],
   "source": [
    "classifier = fasttext.supervised('data.train.txt', 'model')\n",
    "y_pred = [cls[0] for cls in classifier.predict(X_test['text'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-08T16:57:19.275555Z",
     "start_time": "2019-06-08T16:57:19.241035Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score: 0.9668310174809502\n",
      "classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     mchsgov       0.97      0.95      0.96       557\n",
      "         mil       0.98      0.98      0.98       598\n",
      "   mospolice       0.97      0.98      0.97       558\n",
      " russianpost       0.94      0.96      0.95       518\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      2231\n",
      "   macro avg       0.97      0.97      0.97      2231\n",
      "weighted avg       0.97      0.97      0.97      2231\n",
      "\n",
      "confusion_matrix:\n",
      "[[528   7   2  20]\n",
      " [  9 587   0   2]\n",
      " [  1   2 546   9]\n",
      " [  4   3  15 496]]\n"
     ]
    }
   ],
   "source": [
    "print('accuracy_score:', accuracy_score(y_test, y_pred))\n",
    "print('classification_report:', classification_report(y_test, y_pred), sep='\\n')\n",
    "print('confusion_matrix:', confusion_matrix(y_test, y_pred), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-08T16:59:29.485931Z",
     "start_time": "2019-06-08T16:59:29.482295Z"
    }
   },
   "source": [
    "**Наблюдения**\n",
    "- результат FastText сравним с LGBM на tf-idf, но в разы быстрее и требует меньше памяти"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 8 [4 балла]. CNN\n",
    "\n",
    "Реализуйте модель Kim et al (2014) для решения задачи классификации с помощью CNN. Оцените результаты классификации по стандартным мерам качества и проведите анализ ошибок. Для этого рекомендуем визуализировать матрицу ошибок (confusion matrix).\n",
    "Ссылка: Kim Y. Convolutional Neural Networks for Sentence Classification. 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-08T18:15:45.270636Z",
     "start_time": "2019-06-08T18:15:41.178167Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, MaxPooling1D, Convolution1D, Embedding\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "emb_size = 100\n",
    "sequence_length = 50\n",
    "\n",
    "\n",
    "def build_model(w2v_model):\n",
    "    model_input = Input(shape=(sequence_length,))\n",
    "    x = Embedding(len(w2v_model.wv.vocab), emb_size, input_length=sequence_length, name=\"embedding\")(model_input)\n",
    "    x = Dropout(0.2)(x)\n",
    "\n",
    "    conv_blocks = []\n",
    "    for sz in [3, 7]:\n",
    "        conv = Convolution1D(filters=10, kernel_size=sz,  padding=\"valid\", activation=\"relu\", strides=1)(x)\n",
    "        conv = MaxPooling1D()(conv)\n",
    "        conv = Flatten()(conv)\n",
    "        conv_blocks.append(conv)\n",
    "\n",
    "    x = Concatenate()(conv_blocks)\n",
    "\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(50, activation=\"relu\")(x)\n",
    "    model_output = Dense(4, activation=\"softmax\")(x)\n",
    "\n",
    "    model = Model(model_input, model_output)\n",
    "    \n",
    "    # Initialize weights with word2vec\n",
    "    weights = np.array([w2v_model.wv.get_vector(word) for word in w2v_model.wv.vocab.keys()])\n",
    "    embedding_layer = model.get_layer(\"embedding\")\n",
    "    embedding_layer.set_weights([weights])\n",
    "    return model\n",
    "\n",
    "\n",
    "w2v_model = Word2Vec(texts, size=emb_size, window=5, min_count=1, workers=4)\n",
    "model = build_model(w2v_model)\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-08T18:15:45.642967Z",
     "start_time": "2019-06-08T18:15:45.638173Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_15 (InputLayer)           (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 50, 100)      3550500     input_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_25 (Dropout)            (None, 50, 100)      0           embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, 48, 10)       3010        dropout_25[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, 44, 10)       7010        dropout_25[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_25 (MaxPooling1D) (None, 24, 10)       0           conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_26 (MaxPooling1D) (None, 22, 10)       0           conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_25 (Flatten)            (None, 240)          0           max_pooling1d_25[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_26 (Flatten)            (None, 220)          0           max_pooling1d_26[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 460)          0           flatten_25[0][0]                 \n",
      "                                                                 flatten_26[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_26 (Dropout)            (None, 460)          0           concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 50)           23050       dropout_26[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, 4)            204         dense_25[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 3,583,774\n",
      "Trainable params: 3,583,774\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-08T18:15:46.056665Z",
     "start_time": "2019-06-08T18:15:46.046471Z"
    }
   },
   "outputs": [],
   "source": [
    "word_to_idx = {key:idx for idx, key in enumerate(w2v_model.wv.vocab.keys())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-08T18:05:36.879472Z",
     "start_time": "2019-06-08T18:05:36.594896Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train = sequence.pad_sequences([[word_to_idx[word] for word in text.split()] for text in X_train['text']], \n",
    "                                   maxlen=sequence_length, padding=\"post\", truncating=\"post\")\n",
    "x_test = sequence.pad_sequences([[word_to_idx[word] for word in text.split()] for text in X_test['text']], \n",
    "                                  maxlen=sequence_length, padding=\"post\", truncating=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-08T18:18:02.005581Z",
     "start_time": "2019-06-08T18:18:02.001787Z"
    }
   },
   "outputs": [],
   "source": [
    "labels = {'mchsgov': 0, 'mil': 1, 'mospolice': 2, 'russianpost': 3}\n",
    "inv_labels = {0: 'mchsgov', 1: 'mil', 2: 'mospolice', 3: 'russianpost'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-08T18:17:38.175848Z",
     "start_time": "2019-06-08T18:15:49.728291Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8922 samples, validate on 2231 samples\n",
      "Epoch 1/10\n",
      " - 11s - loss: 0.1988 - acc: 0.9255 - val_loss: 0.0963 - val_acc: 0.9752\n",
      "Epoch 2/10\n",
      " - 10s - loss: 0.0932 - acc: 0.9729 - val_loss: 0.0772 - val_acc: 0.9804\n",
      "Epoch 3/10\n",
      " - 10s - loss: 0.0731 - acc: 0.9809 - val_loss: 0.0707 - val_acc: 0.9811\n",
      "Epoch 4/10\n",
      " - 11s - loss: 0.0640 - acc: 0.9822 - val_loss: 0.0706 - val_acc: 0.9818\n",
      "Epoch 5/10\n",
      " - 11s - loss: 0.0548 - acc: 0.9843 - val_loss: 0.0668 - val_acc: 0.9820\n",
      "Epoch 6/10\n",
      " - 11s - loss: 0.0495 - acc: 0.9852 - val_loss: 0.0680 - val_acc: 0.9821\n",
      "Epoch 7/10\n",
      " - 11s - loss: 0.0428 - acc: 0.9873 - val_loss: 0.0646 - val_acc: 0.9842\n",
      "Epoch 8/10\n",
      " - 11s - loss: 0.0361 - acc: 0.9888 - val_loss: 0.0649 - val_acc: 0.9835\n",
      "Epoch 9/10\n",
      " - 11s - loss: 0.0335 - acc: 0.9893 - val_loss: 0.0732 - val_acc: 0.9820\n",
      "Epoch 10/10\n",
      " - 11s - loss: 0.0299 - acc: 0.9906 - val_loss: 0.0662 - val_acc: 0.9827\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f5217da1080>"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, to_categorical([labels[y] for y in y_train], num_classes=4),\n",
    "          validation_data=(x_test, to_categorical([labels[y] for y in y_test], num_classes=4),),\n",
    "          batch_size=batch_size, epochs=num_epochs, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-08T18:18:08.587927Z",
     "start_time": "2019-06-08T18:18:08.395023Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = [inv_labels[y] for y in np.argmax(model.predict(x_test), axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-08T18:18:11.575801Z",
     "start_time": "2019-06-08T18:18:11.540475Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score: 0.9645898700134469\n",
      "classification_report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     mchsgov       0.96      0.95      0.96       557\n",
      "         mil       0.99      0.98      0.98       598\n",
      "   mospolice       0.96      0.99      0.98       558\n",
      " russianpost       0.95      0.93      0.94       518\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      2231\n",
      "   macro avg       0.96      0.96      0.96      2231\n",
      "weighted avg       0.96      0.96      0.96      2231\n",
      "\n",
      "confusion_matrix:\n",
      "[[531   3   1  22]\n",
      " [  7 584   3   4]\n",
      " [  4   0 553   1]\n",
      " [ 12   3  19 484]]\n"
     ]
    }
   ],
   "source": [
    "print('accuracy_score:', accuracy_score(y_test, y_pred))\n",
    "print('classification_report:', classification_report(y_test, y_pred), sep='\\n')\n",
    "print('confusion_matrix:', confusion_matrix(y_test, y_pred), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Наблюдения**\n",
    "- Быстрый рост метрик и высокая точность\n",
    "- Результат лучше, чем у FastText\n",
    "- Можно улучшить перебирая гиперпараметры"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 9 [4 + 2 балла]. RNN\n",
    "\n",
    "(4 балла)Используйте ```RNN``` (BLSTM с какими-то признаками и пулинг поверх) для решения задачи текстовой классификации. Оцените результаты классификации по стандартным мерам качества и проведите анализ ошибок. Для этого рекомендуем визуализировать матрицу ошибок (confusion matrix).\n",
    "\n",
    "За дополнительные 2 балла добавьте в модель символьные признаки - CharCNN или CharRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-08T19:21:59.665754Z",
     "start_time": "2019-06-08T19:21:56.074408Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, LSTM, Dense, TimeDistributed, Dropout, Bidirectional, Input, concatenate\n",
    "from keras.models import Model\n",
    "\n",
    "w2v_model = Word2Vec(texts, size=emb_size, window=5, min_count=1, workers=4)\n",
    "\n",
    "batch_size = 64\n",
    "num_epochs = 10\n",
    "emb_size = 100\n",
    "sequence_length = 50\n",
    "\n",
    "def build_model(w2v_model):\n",
    "    weights = np.array([w2v_model.wv.get_vector(word) for word in w2v_model.wv.vocab.keys()])\n",
    "    \n",
    "    # Вход для BiLSTM\n",
    "    model_input_sent = Input(shape=(sequence_length,))\n",
    "    x = Embedding(len(w2v_model.wv.vocab), emb_size, \n",
    "                  input_length=sequence_length, weights=[weights], \n",
    "                  name=\"embedding\")(model_input_sent)\n",
    "    x = Bidirectional(LSTM(32, return_sequences=True))(x)\n",
    "    x = MaxPooling1D()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(4, activation=\"softmax\")(x)\n",
    "    model = Model(inputs=[model_input_sent], outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-08T19:22:02.549985Z",
     "start_time": "2019-06-08T19:22:00.051464Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_25 (InputLayer)        (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "embedding (Embedding)        (None, 50, 100)           3550500   \n",
      "_________________________________________________________________\n",
      "bidirectional_10 (Bidirectio (None, 50, 64)            34048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 25, 64)            0         \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 25, 64)            0         \n",
      "_________________________________________________________________\n",
      "flatten_27 (Flatten)         (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 4)                 6404      \n",
      "=================================================================\n",
      "Total params: 3,590,952\n",
      "Trainable params: 3,590,952\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(w2v_model)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-06-08T19:24:32.012514Z",
     "start_time": "2019-06-08T19:22:05.420391Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8922 samples, validate on 2231 samples\n",
      "Epoch 1/10\n",
      " - 17s - loss: 0.2881 - acc: 0.9147 - val_loss: 0.1681 - val_acc: 0.9534\n",
      "Epoch 2/10\n",
      " - 15s - loss: 0.1237 - acc: 0.9668 - val_loss: 0.1333 - val_acc: 0.9623\n",
      "Epoch 3/10\n",
      " - 14s - loss: 0.0853 - acc: 0.9770 - val_loss: 0.1219 - val_acc: 0.9677\n",
      "Epoch 4/10\n",
      " - 14s - loss: 0.0611 - acc: 0.9831 - val_loss: 0.1240 - val_acc: 0.9615\n",
      "Epoch 5/10\n",
      " - 14s - loss: 0.0412 - acc: 0.9890 - val_loss: 0.1281 - val_acc: 0.9700\n",
      "Epoch 6/10\n",
      " - 14s - loss: 0.0295 - acc: 0.9920 - val_loss: 0.1513 - val_acc: 0.9659\n",
      "Epoch 7/10\n",
      " - 14s - loss: 0.0205 - acc: 0.9941 - val_loss: 0.1569 - val_acc: 0.9718\n",
      "Epoch 8/10\n",
      " - 14s - loss: 0.0162 - acc: 0.9953 - val_loss: 0.1488 - val_acc: 0.9718\n",
      "Epoch 9/10\n",
      " - 15s - loss: 0.0150 - acc: 0.9957 - val_loss: 0.1678 - val_acc: 0.9704\n",
      "Epoch 10/10\n",
      " - 15s - loss: 0.0098 - acc: 0.9969 - val_loss: 0.1806 - val_acc: 0.9709\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f52194a2198>"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, to_categorical([labels[y] for y in y_train], num_classes=4),\n",
    "          validation_data=(x_test, to_categorical([labels[y] for y in y_test], num_classes=4)),\n",
    "          batch_size=batch_size, epochs=num_epochs, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Наблюдения**\n",
    "- LSTM работает не хуже, чем CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 10 [8 баллов]. ULMFit\n",
    "\n",
    "Используйте ```ULMFit``` для решения задачи классификации. Оцените результаты классификации по стандартным мерам качества и проведите анализ ошибок. Для этого рекомендуем визуализировать матрицу ошибок (confusion matrix). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_8fKqCD6Q5tW"
   },
   "source": [
    "## Конец\n",
    "Выполните какие-то из предыдущих заданий. Для всех заданий, кроме задания 1 требуется вычислить метрику accuracy метода.\n",
    "\n",
    "Подведите итоги и проведите сравнение всех использованных методов. Какой из них показался вам лучше и почему?\n",
    "\n",
    "**NB!** Задание обязательное вне зависимости от того, сколько из предыдущих пунктов вы выполнили, и дополнительных баллов не дает.\n",
    "\n",
    "\n",
    "Для получения полной оценки за NLP-часть достаточно набрать **20 баллов**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Итоги**\n",
    "- Частные сравнения описанны в наблюдениях к каждому заданию.\n",
    "- Лучший из методов с точки зрения простоты - tfidf + classifier. Но она скорее всего будет показывать плохой результат на более сложных данных\n",
    "- Следущий по простоте и лучший по качеству CNN с предобученными эмбэдингами от w2v, которые сами по себе дают плохой результат в связке со стандартным классификатором"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4FkOuR_NiXeT"
   },
   "source": [
    "# Правила сдачи \n",
    "\n",
    "1. Домашняя работа должна быть выполнена в ipynb-тетрадке.\n",
    "2. Сделанную тетрадку нужно отправить ассистенту (ссылка на контакты будет в вики).\n",
    "3. Задание выполняется индивидуально.\n",
    "4. Все вычисления должны быть снабжены пояснениями!\n",
    "5. Дедлайн – 10 июня в 10.00.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HW1.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
